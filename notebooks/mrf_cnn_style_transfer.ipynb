{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oTFmQtJep_Q"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "piQBs1nuenGw"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "import os\n",
    "import PIL\n",
    "import requests\n",
    "import sys\n",
    "from typing import Callable, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import fastai.vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should set the following option to True if the notebook isn't located in the file system inside a clone of the git repo (with the needed Python modules available) it belongs to; i.e., it's running independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_as_standalone_nb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell needs to be executed before importing local project modules, like import genlab.core.gan\n",
    "if run_as_standalone_nb:\n",
    "    root_lib_path = os.path.abspath('generative-lab')\n",
    "    if not os.path.exists(root_lib_path):\n",
    "        !git clone https://github.com/davidleonfdez/generative-lab.git\n",
    "    if root_lib_path not in sys.path:\n",
    "        sys.path.insert(0, root_lib_path)\n",
    "else:\n",
    "    import local_lib_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local project modules. Must be imported after local_lib_import or cloning git repo.\n",
    "from genlab.style_transfer import (denormalize, get_transformed_style_imgs, HyperParams,\n",
    "                                   img_to_tensor, LossWeights, normalize, train, \n",
    "                                   train_progressive_growing, TransformSpecs)\n",
    "from genlab.core.gen_utils import get_img_from_url, PrinterProgressTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cPEp1pdenG1"
   },
   "outputs": [],
   "source": [
    "patch_sz = 3\n",
    "n_ch = 3\n",
    "img_sz = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRRVKhLkenHG"
   },
   "source": [
    "# INPUTS MANAGEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iarCyDlnenHH"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/mf1024/ImageNet-Datasets-Downloader.git \"C:/Users/blabla/ImageNetDownloader\"\n",
    "def download_imagenet_subset():\n",
    "    !python C:/Users/blabla/ImageNetDownloader/downloader.py \\\n",
    "        -data_root C:/Users/blabla/imagenet \\\n",
    "        -number_of_classes 5 \\\n",
    "        -images_per_class 10    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XijATYk_enHJ"
   },
   "outputs": [],
   "source": [
    "def img_t_from_url(url, target_sz:int=img_sz) -> torch.Tensor:\n",
    "    img = get_img_from_url(url)\n",
    "    return img_to_tensor(img, target_sz)\n",
    "    \n",
    "def img_t_from_path(path) -> torch.Tensor:\n",
    "    img = PIL.Image.open(path)\n",
    "    return img_to_tensor(img, img_sz)\n",
    "\n",
    "def check_norm_is_needed_vgg(x):\n",
    "    stats = (torch.Tensor([0.485, 0.456, 0.406]), \n",
    "             torch.Tensor([0.229, 0.224, 0.225]))\n",
    "    out = vgg(fastai.vision.normalize(x, *stats))\n",
    "    out2 = vgg(x)\n",
    "    return out.max(), out.argmax(), out2.max(), out2.argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageURLs:\n",
    "    PAINTING = 'https://www.moma.org/media/W1siZiIsIjQ2NzUxNyJdLFsicCIsImNvbnZlcnQiLCItcXVhbGl0eSA5MCAtcmVzaXplIDIwMDB4MjAwMFx1MDAzZSJdXQ.jpg?sha=314ebf8cc678676f'\n",
    "    BASKET_BALL = 'https://miro.medium.com/proxy/1*BDE-SkJBCG_7P4chK4vKnw.jpeg'\n",
    "    FERRARI_F1 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Ferrari_F1_2006_EMS.jpg/1024px-Ferrari_F1_2006_EMS.jpg'\n",
    "    RENAULT_F1 = 'https://upload.wikimedia.org/wikipedia/commons/3/31/Renault_F1_front_IAA_2005.jpg'\n",
    "    ELON_MUSK = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Elon_Musk_2015.jpg/408px-Elon_Musk_2015.jpg'\n",
    "    # By Guillaume Paumier: https://www.flickr.com/photos/gpaumier/6198197101\n",
    "    MARK_ZUCK = 'https://live.staticflickr.com/6156/6198197101_9d7a685618_b.jpg'\n",
    "    PICASSO_RETRATO = 'https://live.staticflickr.com/4003/4407941037_9718d307da_b.jpg'\n",
    "    PAPER_GIRL = 'https://live.staticflickr.com/8595/16020558165_1ed9f5af8c_b.jpg'\n",
    "    PAPER_GIRL_PIC = 'https://live.staticflickr.com/1/2281680_656225393e_c.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_img_t = normalize(img_t_from_url(ImageURLs.PAINTING))\n",
    "basket_ball_img_t = normalize(img_t_from_url(ImageURLs.BASKET_BALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai.vision.show_image(denormalize(basket_ball_img_t[0]))\n",
    "fastai.vision.show_image(denormalize(painting_img_t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_imgs_t = normalize(get_transformed_style_imgs(get_img_from_url(ImageURLs.PAINTING)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcXkSPQ1enHP"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZdXvCGpX6eH"
   },
   "outputs": [],
   "source": [
    "def get_print_results_callback(n_iters_between:int, n_total_iters:int, show_sz=False):\n",
    "    n_imgs = n_total_iters // n_iters_between\n",
    "    n_cols = 3\n",
    "    n_rows = n_imgs//n_cols + min(1, n_imgs % n_cols)\n",
    "    imgs, losses = [], []\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 16 * n_rows/n_cols))\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs.flatten(): ax.axis('off')\n",
    "    display_id = None\n",
    "\n",
    "    def _print_result(i, gen_img_t, loss):\n",
    "        if (i+1) % n_iters_between == 0:\n",
    "            gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "            gen_img = fastai.vision.Image(gen_img_t)          \n",
    "            imgs.append(gen_img)\n",
    "            losses.append(loss.detach().cpu())\n",
    "            for j, img in enumerate(imgs):\n",
    "                # If using progressive growing, it's only right if the number of iterations \n",
    "                # by size is a multiple of `n_iters_between`\n",
    "                iter_idx = (j+1) * n_iters_between\n",
    "                title = f'Iteration {iter_idx}, loss = {losses[j]}'\n",
    "                if show_sz: title += f', size = {img.size}'\n",
    "                img.show(ax=axs[j], title=title)\n",
    "            #plt.close()\n",
    "            nonlocal display_id\n",
    "            if display_id is None: display_id = display.display(fig, display_id=True)\n",
    "            display_id.update(fig)\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "    return _print_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "P3HaOyfrhgRS",
    "outputId": "b0a83833-25b6-4ecf-da49-4ffc96a1402f"
   },
   "outputs": [],
   "source": [
    "n_iters=200\n",
    "gen_img_t = train(painting_img_t, \n",
    "                  basket_ball_img_t,\n",
    "                  hyperparams=HyperParams(lr=0.1), \n",
    "                  n_iters=n_iters, \n",
    "                  loss_weights=LossWeights(style=0.1, content=1., reg=0.),\n",
    "                  callbacks=[get_print_results_callback(20, n_iters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "kvjBe3G7enHR",
    "outputId": "d53ea96a-c8ed-450b-e87e-c5e9d7de2360"
   },
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpoRRk_k244B"
   },
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "D0RwdA4r2mc2",
    "outputId": "7c9ea08b-be35-4f2f-de6f-b28741109151"
   },
   "outputs": [],
   "source": [
    "gen_img_t.min(), gen_img_t.max(), gen_img_t.size(), gen_img_t.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wqejd6ogB1hK",
    "outputId": "eb38fd3f-75be-413b-d849-1886c0175723"
   },
   "outputs": [],
   "source": [
    "denorm_gen_img_t = denormalize(gen_img_t.detach())\n",
    "pix_dist = {0: 0}\n",
    "for px in denorm_gen_img_t.flatten():\n",
    "    if 0. <= px <= 1.: \n",
    "        pix_dist[0] += 1\n",
    "    else:\n",
    "        key = int(px.item()//1)\n",
    "        if key not in pix_dist: pix_dist[key] = 0\n",
    "        pix_dist[key] += 1\n",
    "\n",
    "pix_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some pixels out of range. Maybe they would be black/white anyway and thinking about a solution (like penalizing it with loss func) is not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters=200\n",
    "gen_img_t = train(painting_img_t, \n",
    "                  basket_ball_img_t,\n",
    "                  hyperparams=HyperParams(lr=0.1), \n",
    "                  n_iters=n_iters, \n",
    "                  loss_weights=LossWeights(style=0.1, content=1., reg=0.),\n",
    "                  callbacks=[get_print_results_callback(20, n_iters)],\n",
    "                  device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with progressive growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = get_img_from_url(ImageURLs.PAINTING)\n",
    "content_img = get_img_from_url(ImageURLs.BASKET_BALL)\n",
    "n_iters_by_sz = 200\n",
    "init_sz, target_sz = 64, 256\n",
    "n_total_iters = n_iters_by_sz * int(1 + math.log2(target_sz//init_sz))\n",
    "gen_img_t = train_progressive_growing(style_img, \n",
    "                                      content_img, \n",
    "                                      target_sz,\n",
    "                                      init_sz=init_sz,\n",
    "                                      style_img_tfms=TransformSpecs.none(),\n",
    "                                      n_iters_by_sz=n_iters_by_sz, \n",
    "                                      hyperparams=HyperParams(lr=0.1),\n",
    "                                      loss_weights=LossWeights(style=0.1, content=1., reg=1e-3),\n",
    "                                      callbacks=[get_print_results_callback(40, n_total_iters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use several transformed versions of the style image, just omit the param `style_img_tfms` of `train_progressive_growing` or pass the rotations and scales especifically to the constructor of `TransformSpecs`. For example:\n",
    "\n",
    "```\n",
    "   style_img_tfms=TransformSpecs(do_scale=True, \n",
    "                                 do_rotate=True, \n",
    "                                 scales=(0.975, 1.), \n",
    "                                 rotations=(-3., 0, 3.))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it_pg64-256.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhAYuNRTenHf"
   },
   "source": [
    "# PENDING\n",
    "\n",
    "* Penalize gen_img being out of range (be aware range is different for each channel)\n",
    "* Check better if normalized cross-correlation is ok\n",
    "* Don't forget to release plots, be it plot.close() or whatever\n",
    "* How to deal with rectangular images? Right now, I'm resizing the larger dim to 224 and filling the rest with black padding. Other options are:\n",
    "  * Crop (even if random it shouldn't make much sense)\n",
    "  * Check/think if vgg19 is actually capable of dealing with input sizes other than (224, 224) while preserving evaluation quality\n",
    "* Fit one cycle may be used/adapted????\n",
    "* Add a reference to the paper https://arxiv.org/pdf/1601.04589.pdf and authors\n",
    "  \n",
    "# TRAINING TEST STEPS\n",
    "1. Use only content loss\n",
    "2. Check adding style loss improves the results\n",
    "    -Check requires_grad of tensors in the middle of the process\n",
    "3. Check adding regularizer smoothes the result\n",
    "\n",
    "# IMPROVEMENTS\n",
    "* Precalc as much stuff related to content and style image as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYo1iFmlenHf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gjw4-bKLUEAX",
    "BwbNdBVHNJff",
    "46I-sgt54f5A",
    "HGGwo7J29YWF",
    "Y6JeXXBLJlWh",
    "kHNcWMMaenHT",
    "Kmd4GyjpenHV"
   ],
   "name": "mrf_cnn_style_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "321a24087ca24c30a7a1fbb70522aa78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "63d193063c974ce38dd5658c1003f934": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7003d257b40d43468dedf56fbeac7d03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7263955811f047c0a17cb5cb242c79c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "804e589b8a5e4778a3c717017b0ab141": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f0ac62206cff454589d29df65f91f33f",
       "IPY_MODEL_8f512a2b726246b08d3bf4ab107c91da"
      ],
      "layout": "IPY_MODEL_b54f703adf1c48508f349610fa4eb11d"
     }
    },
    "8f512a2b726246b08d3bf4ab107c91da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7263955811f047c0a17cb5cb242c79c6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7003d257b40d43468dedf56fbeac7d03",
      "value": " 548M/548M [01:33&lt;00:00, 6.18MB/s]"
     }
    },
    "b54f703adf1c48508f349610fa4eb11d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0ac62206cff454589d29df65f91f33f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63d193063c974ce38dd5658c1003f934",
      "max": 574673361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_321a24087ca24c30a7a1fbb70522aa78",
      "value": 574673361
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
