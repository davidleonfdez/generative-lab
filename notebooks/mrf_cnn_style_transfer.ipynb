{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oTFmQtJep_Q"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "piQBs1nuenGw"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "import os\n",
    "import PIL\n",
    "import requests\n",
    "import sys\n",
    "from typing import Callable, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import fastai.vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should set the following option to True if the notebook isn't located in the file system inside a clone of the git repo (with the needed Python modules available) it belongs to; i.e., it's running independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_as_standalone_nb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell needs to be executed before importing local project modules, like import genlab.core.gan\n",
    "if run_as_standalone_nb:\n",
    "    root_lib_path = os.path.abspath('generative-lab')\n",
    "    if not os.path.exists(root_lib_path):\n",
    "        !git clone https://github.com/davidleonfdez/generative-lab.git\n",
    "    if root_lib_path not in sys.path:\n",
    "        sys.path.insert(0, root_lib_path)\n",
    "else:\n",
    "    import local_lib_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local project modules. Must be imported after local_lib_import or cloning git repo.\n",
    "from genlab.style_transfer import (denormalize, get_transformed_style_imgs, HyperParams,\n",
    "                                   img_to_tensor, LossWeights, normalize, train, \n",
    "                                   train_progressive_growing, TransformSpecs)\n",
    "from genlab.core.gen_utils import PrinterProgressTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cPEp1pdenG1"
   },
   "outputs": [],
   "source": [
    "patch_sz = 3\n",
    "n_ch = 3\n",
    "img_sz = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRRVKhLkenHG"
   },
   "source": [
    "# INPUTS MANAGEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iarCyDlnenHH"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/mf1024/ImageNet-Datasets-Downloader.git \"C:/Users/blabla/ImageNetDownloader\"\n",
    "def download_imagenet_subset():\n",
    "    !python C:/Users/blabla/ImageNetDownloader/downloader.py \\\n",
    "        -data_root C:/Users/blabla/imagenet \\\n",
    "        -number_of_classes 5 \\\n",
    "        -images_per_class 10\n",
    "    \n",
    "def get_img_from_url(url) -> PIL.Image.Image:\n",
    "    import requests\n",
    "    return PIL.Image.open(requests.get(url, stream=True).raw)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XijATYk_enHJ"
   },
   "outputs": [],
   "source": [
    "def img_t_from_url(url, target_sz:int=img_sz) -> torch.Tensor:\n",
    "    img = get_img_from_url(url)\n",
    "    return img_to_tensor(img, target_sz)\n",
    "    \n",
    "def img_t_from_path(path) -> torch.Tensor:\n",
    "    img = PIL.Image.open(path)\n",
    "    return img_to_tensor(img, img_sz)\n",
    "\n",
    "def check_norm_is_needed_vgg(x):\n",
    "    stats = (torch.Tensor([0.485, 0.456, 0.406]), \n",
    "             torch.Tensor([0.229, 0.224, 0.225]))\n",
    "    out = vgg(fastai.vision.normalize(x, *stats))\n",
    "    out2 = vgg(x)\n",
    "    return out.max(), out.argmax(), out2.max(), out2.argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageURLs:\n",
    "    PAINTING = 'https://www.moma.org/media/W1siZiIsIjQ2NzUxNyJdLFsicCIsImNvbnZlcnQiLCItcXVhbGl0eSA5MCAtcmVzaXplIDIwMDB4MjAwMFx1MDAzZSJdXQ.jpg?sha=314ebf8cc678676f'\n",
    "    BASKET_BALL = 'https://miro.medium.com/proxy/1*BDE-SkJBCG_7P4chK4vKnw.jpeg'\n",
    "    FERRARI_F1 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Ferrari_F1_2006_EMS.jpg/1024px-Ferrari_F1_2006_EMS.jpg'\n",
    "    RENAULT_F1 = 'https://upload.wikimedia.org/wikipedia/commons/3/31/Renault_F1_front_IAA_2005.jpg'\n",
    "    ELON_MUSK = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Elon_Musk_2015.jpg/408px-Elon_Musk_2015.jpg'\n",
    "    # By Guillaume Paumier: https://www.flickr.com/photos/gpaumier/6198197101\n",
    "    MARK_ZUCK = 'https://live.staticflickr.com/6156/6198197101_9d7a685618_b.jpg'\n",
    "    PICASSO_RETRATO = 'https://live.staticflickr.com/4003/4407941037_9718d307da_b.jpg'\n",
    "    PAPER_GIRL = 'https://live.staticflickr.com/8595/16020558165_1ed9f5af8c_b.jpg'\n",
    "    PAPER_GIRL_PIC = 'https://live.staticflickr.com/1/2281680_656225393e_c.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_img_t = normalize(img_t_from_url(ImageURLs.PAINTING))\n",
    "basket_ball_img_t = normalize(img_t_from_url(ImageURLs.BASKET_BALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai.vision.show_image(denormalize(basket_ball_img_t[0]))\n",
    "fastai.vision.show_image(denormalize(painting_img_t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_imgs_t = normalize(get_transformed_style_imgs(get_img_from_url(ImageURLs.PAINTING)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcXkSPQ1enHP"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZdXvCGpX6eH"
   },
   "outputs": [],
   "source": [
    "def get_print_results_callback(n_iters_between:int, n_total_iters:int, show_sz=False):\n",
    "    n_imgs = n_total_iters // n_iters_between\n",
    "    n_cols = 3\n",
    "    n_rows = n_imgs//n_cols + min(1, n_imgs % n_cols)\n",
    "    imgs, losses = [], []\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 16 * n_rows/n_cols))\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs.flatten(): ax.axis('off')\n",
    "    display_id = None\n",
    "\n",
    "    def _print_result(i, gen_img_t, loss):\n",
    "        if (i+1) % n_iters_between == 0:\n",
    "            gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "            gen_img = fastai.vision.Image(gen_img_t)          \n",
    "            imgs.append(gen_img)\n",
    "            losses.append(loss.detach().cpu())\n",
    "            for j, img in enumerate(imgs):\n",
    "                # If using progressive growing, it's only right if the number of iterations \n",
    "                # by size is a multiple of `n_iters_between`\n",
    "                iter_idx = (j+1) * n_iters_between\n",
    "                title = f'Iteration {iter_idx}, loss = {losses[j]}'\n",
    "                if show_sz: title += f', size = {img.size}'\n",
    "                img.show(ax=axs[j], title=title)\n",
    "            #plt.close()\n",
    "            nonlocal display_id\n",
    "            if display_id is None: display_id = display.display(fig, display_id=True)\n",
    "            display_id.update(fig)\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "    return _print_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "P3HaOyfrhgRS",
    "outputId": "b0a83833-25b6-4ecf-da49-4ffc96a1402f"
   },
   "outputs": [],
   "source": [
    "n_iters=200\n",
    "gen_img_t = train(painting_img_t, \n",
    "                  basket_ball_img_t,\n",
    "                  hyperparams=HyperParams(lr=0.1), \n",
    "                  n_iters=n_iters, \n",
    "                  loss_weights=LossWeights(style=0.1, content=1., reg=0.),\n",
    "                  callbacks=[get_print_results_callback(20, n_iters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "kvjBe3G7enHR",
    "outputId": "d53ea96a-c8ed-450b-e87e-c5e9d7de2360"
   },
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpoRRk_k244B"
   },
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "D0RwdA4r2mc2",
    "outputId": "7c9ea08b-be35-4f2f-de6f-b28741109151"
   },
   "outputs": [],
   "source": [
    "gen_img_t.min(), gen_img_t.max(), gen_img_t.size(), gen_img_t.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wqejd6ogB1hK",
    "outputId": "eb38fd3f-75be-413b-d849-1886c0175723"
   },
   "outputs": [],
   "source": [
    "denorm_gen_img_t = denormalize(gen_img_t.detach())\n",
    "pix_dist = {0: 0}\n",
    "for px in denorm_gen_img_t.flatten():\n",
    "    if 0. <= px <= 1.: \n",
    "        pix_dist[0] += 1\n",
    "    else:\n",
    "        key = int(px.item()//1)\n",
    "        if key not in pix_dist: pix_dist[key] = 0\n",
    "        pix_dist[key] += 1\n",
    "\n",
    "pix_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some pixels out of range. Maybe they would be black/white anyway and thinking about a solution (like penalizing it with loss func) is not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters=200\n",
    "gen_img_t = train(painting_img_t, \n",
    "                  basket_ball_img_t,\n",
    "                  hyperparams=HyperParams(lr=0.1), \n",
    "                  n_iters=n_iters, \n",
    "                  loss_weights=LossWeights(style=0.1, content=1., reg=0.),\n",
    "                  callbacks=[get_print_results_callback(20, n_iters)],\n",
    "                  device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with progressive growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = get_img_from_url(ImageURLs.PAINTING)\n",
    "content_img = get_img_from_url(ImageURLs.BASKET_BALL)\n",
    "n_iters_by_sz = 200\n",
    "init_sz, target_sz = 64, 256\n",
    "n_total_iters = n_iters_by_sz * int(1 + math.log2(target_sz//init_sz))\n",
    "gen_img_t = train_progressive_growing(style_img, \n",
    "                                      content_img, \n",
    "                                      target_sz,\n",
    "                                      init_sz=init_sz,\n",
    "                                      style_img_tfms=TransformSpecs.none(),\n",
    "                                      n_iters_by_sz=n_iters_by_sz, \n",
    "                                      hyperparams=HyperParams(lr=0.1),\n",
    "                                      loss_weights=LossWeights(style=0.1, content=1., reg=1e-3),\n",
    "                                      callbacks=[get_print_results_callback(40, n_total_iters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use several transformed versions of the style image, just omit the param `style_img_tfms` of `train_progressive_growing` or pass the rotations and scales especifically to the constructor of `TransformSpecs`. For example:\n",
    "\n",
    "```\n",
    "   style_img_tfms=TransformSpecs(do_scale=True, \n",
    "                                 do_rotate=True, \n",
    "                                 scales=(0.975, 1.), \n",
    "                                 rotations=(-3., 0, 3.))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it_pg64-256.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHNcWMMaenHT"
   },
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbhhFA0DenHT",
    "outputId": "81b6163b-a64f-402f-f9f1-ee3f7ac4d2a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_content_loss():\n",
    "    x = torch.Tensor([[[0]*4]*4]*3)\n",
    "    y = torch.Tensor([[[1]*4]*4]*3)\n",
    "    loss1 = content_loss(x, y)\n",
    "    x += 0.5\n",
    "    loss2 = content_loss(x, y)\n",
    "    y -= 0.25\n",
    "    loss3 = content_loss(x, y)\n",
    "    assert loss1 == 4*4*3\n",
    "    assert loss2 == 4*3\n",
    "    assert loss3 == 3\n",
    "\n",
    "def test_smoothness_reg():\n",
    "    uniform_img = torch.Tensor([[[1]*4]*4]*3)\n",
    "    print(uniform_img.size())\n",
    "    diffy_x_img = torch.Tensor([[[1]*4, [0]*4]*2]*3)\n",
    "    diffy_y_img = torch.Tensor([[[1, 0]*2]*4]*3)\n",
    "    diffy_x_y_img = torch.Tensor([[[1, 0]*2, [0, 1]*2]*2]*3)\n",
    "    \n",
    "    print(diffy_x_y_img)\n",
    "    assert smoothness_reg(uniform_img) == 0\n",
    "    assert smoothness_reg(diffy_x_img) == 3*3*4\n",
    "    assert smoothness_reg(diffy_y_img) == 3*4*3\n",
    "    assert smoothness_reg(diffy_x_y_img) == 2*3*4*3\n",
    "\n",
    "def test_split_in_patches():\n",
    "    img = torch.Tensor([i for i in range(3*4*4)]).view(3, 4, 4)\n",
    "    # tensor([[[ 0.,  1.,  2.,  3.],\n",
    "    #          [ 4.,  5.,  6.,  7.],\n",
    "    #          [ 8.,  9., 10., 11.],\n",
    "    #          [12., 13., 14., 15.]],\n",
    "\n",
    "    #         [[16., 17., 18., 19.],\n",
    "    #          [20., 21., 22., 23.],\n",
    "    #          [24., 25., 26., 27.],\n",
    "    #          [28., 29., 30., 31.]],\n",
    "\n",
    "    #         [[32., 33., 34., 35.],\n",
    "    #          [36., 37., 38., 39.],\n",
    "    #          [40., 41., 42., 43.],\n",
    "    #          [44., 45., 46., 47.]]])\n",
    "    actual_2x2 = split_in_patches(img, patch_sz=2)\n",
    "    actual_3x3 = split_in_patches(img, patch_sz=3)\n",
    "    expected_2x2 = torch.Tensor(\n",
    "       [[[[ 0.,  1.],\n",
    "          [ 4.,  5.]],\n",
    "\n",
    "         [[16., 17.],\n",
    "          [20., 21.]],\n",
    "\n",
    "         [[32., 33.],\n",
    "          [36., 37.]]],\n",
    "\n",
    "\n",
    "        [[[ 1.,  2.],\n",
    "          [ 5.,  6.]],\n",
    "\n",
    "         [[17., 18.],\n",
    "          [21., 22.]],\n",
    "\n",
    "         [[33., 34.],\n",
    "          [37., 38.]]],\n",
    "\n",
    "\n",
    "        [[[ 2.,  3.],\n",
    "          [ 6.,  7.]],\n",
    "\n",
    "         [[18., 19.],\n",
    "          [22., 23.]],\n",
    "\n",
    "         [[34., 35.],\n",
    "          [38., 39.]]],\n",
    "\n",
    "\n",
    "        [[[ 4.,  5.],\n",
    "          [ 8.,  9.]],\n",
    "\n",
    "         [[20., 21.],\n",
    "          [24., 25.]],\n",
    "\n",
    "         [[36., 37.],\n",
    "          [40., 41.]]],\n",
    "\n",
    "\n",
    "        [[[ 5.,  6.],\n",
    "          [ 9., 10.]],\n",
    "\n",
    "         [[21., 22.],\n",
    "          [25., 26.]],\n",
    "\n",
    "         [[37., 38.],\n",
    "          [41., 42.]]],\n",
    "\n",
    "\n",
    "        [[[ 6.,  7.],\n",
    "          [10., 11.]],\n",
    "\n",
    "         [[22., 23.],\n",
    "          [26., 27.]],\n",
    "\n",
    "         [[38., 39.],\n",
    "          [42., 43.]]],\n",
    "\n",
    "\n",
    "        [[[ 8.,  9.],\n",
    "          [12., 13.]],\n",
    "\n",
    "         [[24., 25.],\n",
    "          [28., 29.]],\n",
    "\n",
    "         [[40., 41.],\n",
    "          [44., 45.]]],\n",
    "\n",
    "\n",
    "        [[[ 9., 10.],\n",
    "          [13., 14.]],\n",
    "\n",
    "         [[25., 26.],\n",
    "          [29., 30.]],\n",
    "\n",
    "         [[41., 42.],\n",
    "          [45., 46.]]],\n",
    "\n",
    "\n",
    "        [[[10., 11.],\n",
    "          [14., 15.]],\n",
    "\n",
    "         [[26., 27.],\n",
    "          [30., 31.]],\n",
    "\n",
    "         [[42., 43.],\n",
    "          [46., 47.]]]])\n",
    "    expected_3x3=torch.Tensor(\n",
    "       [[[[ 0.,  1.,  2.],\n",
    "          [ 4.,  5.,  6.],\n",
    "          [ 8.,  9., 10.]],\n",
    "\n",
    "         [[16., 17., 18.],\n",
    "          [20., 21., 22.],\n",
    "          [24., 25., 26.]],\n",
    "\n",
    "         [[32., 33., 34.],\n",
    "          [36., 37., 38.],\n",
    "          [40., 41., 42.]]],\n",
    "\n",
    "\n",
    "        [[[ 1.,  2.,  3.],\n",
    "          [ 5.,  6.,  7.],\n",
    "          [ 9., 10., 11.]],\n",
    "\n",
    "         [[17., 18., 19.],\n",
    "          [21., 22., 23.],\n",
    "          [25., 26., 27.]],\n",
    "\n",
    "         [[33., 34., 35.],\n",
    "          [37., 38., 39.],\n",
    "          [41., 42., 43.]]],\n",
    "\n",
    "\n",
    "        [[[ 4.,  5.,  6.],\n",
    "          [ 8.,  9., 10.],\n",
    "          [12., 13., 14.]],\n",
    "\n",
    "         [[20., 21., 22.],\n",
    "          [24., 25., 26.],\n",
    "          [28., 29., 30.]],\n",
    "\n",
    "         [[36., 37., 38.],\n",
    "          [40., 41., 42.],\n",
    "          [44., 45., 46.]]],\n",
    "\n",
    "\n",
    "        [[[ 5.,  6.,  7.],\n",
    "          [ 9., 10., 11.],\n",
    "          [13., 14., 15.]],\n",
    "\n",
    "         [[21., 22., 23.],\n",
    "          [25., 26., 27.],\n",
    "          [29., 30., 31.]],\n",
    "\n",
    "         [[37., 38., 39.],\n",
    "          [41., 42., 43.],\n",
    "          [45., 46., 47.]]]])\n",
    "    assert(torch.equal(actual_2x2, expected_2x2))\n",
    "    assert(torch.equal(actual_3x3, expected_3x3))\n",
    "\n",
    "def test_style_loss(vgg19):\n",
    "    elon_photo_url = 'https://wonderfulengineering.com/wp-content/uploads/2018/09/musk5.jpg'\n",
    "    elon_drawing_url = 'https://i.redd.it/ofljkrzi82r21.jpg'\n",
    "    rectangular_spain_flag_url = 'https://upload.wikimedia.org/wikipedia/commons/d/d5/Flag_of_Spain_%28WFB_2000%29.jpg'\n",
    "    #'http://icons.iconarchive.com/icons/wikipedia/flags/256/ES-Spain-Flag-icon.png'\n",
    "    #hand_spain_flag_url = 'https://cdn.pixabay.com/photo/2015/02/16/00/20/spain-637843_960_720.jpg'\n",
    "    circular_spain_flag_url = 'https://cdn.pixabay.com/photo/2017/10/04/10/44/spain-2815785_960_720.jpg'\n",
    "    #'http://files.softicons.com/download/web-icons/world-cup-flags-icons-by-custom-icon-design/png/64x64/Spain.png'\n",
    "    austria_flag_url = 'https://pixnio.com/free-images/flags-of-the-world/flag-of-austria-725x483.jpg'\n",
    "    #'https://www.publicdomainpictures.net/pictures/120000/velka/austria-flag.jpg'\n",
    "    ftrs_calc = FeaturesCalculator(vgg_style_layers_idx, vgg_content_layers_idx, vgg19)\n",
    "    url_to_ftrs = lambda url: ftrs_calc.calc_style(img_t_from_url(url))\n",
    "\n",
    "    ftr_map_1 = torch.rand(1, 16, 8, 8)\n",
    "    ftr_map_2 = torch.rand(1, 16, 8, 8)    \n",
    "    ftr_map_sp_rect = url_to_ftrs(rectangular_spain_flag_url)[0]\n",
    "    ftr_map_sp_circ = url_to_ftrs(circular_spain_flag_url)[0]\n",
    "    ftr_map_austria = url_to_ftrs(austria_flag_url)[0]\n",
    "    ftr_map_sp_rect_patches = split_in_patches(ftr_map_sp_rect)    \n",
    "    \n",
    "    loss_rect_circ = style_loss(ftr_map_sp_circ, ftr_map_sp_rect, ftr_map_sp_rect_patches)\n",
    "    loss_rect_sp_aust = style_loss(ftr_map_austria, ftr_map_sp_rect, ftr_map_sp_rect_patches)\n",
    "    loss_equal = style_loss(ftr_map_1, ftr_map_1)\n",
    "    loss_different = style_loss(ftr_map_1, ftr_map_2)\n",
    "    #assert loss_equal == 0, f'{loss_equal}'\n",
    "    #assert loss_different > 0, f'{loss_different}'\n",
    "    assert loss_rect_circ < loss_rect_sp_aust, f'{loss_rect_circ}, {loss_rect_sp_aust}'\n",
    "\n",
    "test_content_loss()\n",
    "test_smoothness_reg()\n",
    "test_split_in_patches()\n",
    "# This isn't so easy to test\n",
    "#test_style_loss(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhAYuNRTenHf"
   },
   "source": [
    "# PENDING\n",
    "\n",
    "* Penalize gen_img being out of range (be aware range is different for each channel)\n",
    "* Check better if normalized cross-correlation is ok\n",
    "* Don't forget to release plots, be it plot.close() or whatever\n",
    "* How to deal with rectangular images? Right now, I'm resizing the larger dim to 224 and filling the rest with black padding. Other options are:\n",
    "  * Crop (even if random it shouldn't make much sense)\n",
    "  * Check/think if vgg19 is actually capable of dealing with input sizes other than (224, 224) while preserving evaluation quality\n",
    "* Fit one cycle may be used/adapted????\n",
    "* Add a reference to the paper https://arxiv.org/pdf/1601.04589.pdf and authors\n",
    "  \n",
    "# TRAINING TEST STEPS\n",
    "1. Use only content loss\n",
    "2. Check adding style loss improves the results\n",
    "    -Check requires_grad of tensors in the middle of the process\n",
    "3. Check adding regularizer smoothes the result\n",
    "\n",
    "# IMPROVEMENTS\n",
    "* Precalc as much stuff related to content and style image as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYo1iFmlenHf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gjw4-bKLUEAX",
    "BwbNdBVHNJff",
    "46I-sgt54f5A",
    "HGGwo7J29YWF",
    "Y6JeXXBLJlWh",
    "kHNcWMMaenHT",
    "Kmd4GyjpenHV"
   ],
   "name": "mrf_cnn_style_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "321a24087ca24c30a7a1fbb70522aa78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "63d193063c974ce38dd5658c1003f934": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7003d257b40d43468dedf56fbeac7d03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7263955811f047c0a17cb5cb242c79c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "804e589b8a5e4778a3c717017b0ab141": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f0ac62206cff454589d29df65f91f33f",
       "IPY_MODEL_8f512a2b726246b08d3bf4ab107c91da"
      ],
      "layout": "IPY_MODEL_b54f703adf1c48508f349610fa4eb11d"
     }
    },
    "8f512a2b726246b08d3bf4ab107c91da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7263955811f047c0a17cb5cb242c79c6",
      "placeholder": "​",
      "style": "IPY_MODEL_7003d257b40d43468dedf56fbeac7d03",
      "value": " 548M/548M [01:33&lt;00:00, 6.18MB/s]"
     }
    },
    "b54f703adf1c48508f349610fa4eb11d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0ac62206cff454589d29df65f91f33f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63d193063c974ce38dd5658c1003f934",
      "max": 574673361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_321a24087ca24c30a7a1fbb70522aa78",
      "value": 574673361
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
