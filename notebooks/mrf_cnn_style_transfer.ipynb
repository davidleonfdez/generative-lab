{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oTFmQtJep_Q"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "piQBs1nuenGw"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from IPython import display\n",
    "import math\n",
    "import PIL\n",
    "import requests\n",
    "from typing import Callable, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.vgg import vgg19\n",
    "import torchvision.transforms.functional as TF\n",
    "from fastai.callbacks import hook_outputs\n",
    "import fastai.vision.transform as fastTF\n",
    "import fastai.vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14rVkUzQenGz"
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class ProgressTracker(ABC):\n",
    "    @abstractmethod\n",
    "    def notify(self, message:str):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PrinterProgressTracker(ProgressTracker):\n",
    "    def notify(self, message:str):\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cPEp1pdenG1"
   },
   "outputs": [],
   "source": [
    "patch_sz = 3\n",
    "n_ch = 3\n",
    "img_sz = 224\n",
    "vgg_content_layers_idx = [22]\n",
    "vgg_style_layers_idx = [11, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGTgXEHGenG3"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LossWeights:\n",
    "    style:float=1.\n",
    "    content:float=1.\n",
    "    reg:float=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmhAT8jjenG5"
   },
   "outputs": [],
   "source": [
    "def content_loss(output, target):\n",
    "    return torch.norm(output - target)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XdF1c77enG9"
   },
   "outputs": [],
   "source": [
    "def smoothness_reg(gen_img:torch.Tensor) -> torch.Tensor:\n",
    "    # Equivalent to sum_i,j[(x[i][j+1] - x[i][j])**2 + (x[i+1][j] - x[i][j])**2]\n",
    "    rows_diff = ((gen_img[:,1:] - gen_img[:,0:-1])**2).sum()\n",
    "    cols_diff = ((gen_img[:,:,1:] - gen_img[:,:,0:-1])**2).sum()\n",
    "    return rows_diff + cols_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aGdwLkUenG_"
   },
   "outputs": [],
   "source": [
    "def split_in_patches(t, patch_sz=3) -> torch.Tensor:\n",
    "    rank = len(t.size())  \n",
    "    assert rank in (3, 4), 'Input must be a rank 3 or 4 tensor' \n",
    "    if rank == 3: t = t.unsqueeze(0)\n",
    "    stride = 1\n",
    "    bs, n_ftrs = t.size()[0:2]\n",
    "    return (t.unfold(0, bs, bs)\n",
    "             .unfold(1, n_ftrs, n_ftrs)\n",
    "             .unfold(2, patch_sz, stride)\n",
    "             .unfold(3, patch_sz, stride)\n",
    "             .reshape(-1, bs, n_ftrs, patch_sz, patch_sz)\n",
    "             # Permute first two dims to have all patches from first element of the batch,\n",
    "             # then all patches from second element of the batch, and so on...\n",
    "             .permute(1, 0, 2, 3, 4)\n",
    "             .reshape(-1, n_ftrs, patch_sz, patch_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKnT9_amenHB"
   },
   "outputs": [],
   "source": [
    "def style_loss(gen_ftrs, style_ftrs, precalc_style_patches=None):\n",
    "    style_patches = (split_in_patches(style_ftrs, patch_sz) if precalc_style_patches is None \n",
    "                     else precalc_style_patches)\n",
    "    # n_style_patches will be greater than n_gen_patches when there are several versions\n",
    "    # (probably transforms) of the style image, so that style_ftrs.size()[0] > 1\n",
    "    n_style_patches, n_ftrs = style_patches.size()[0:2]\n",
    "    gen_patches = split_in_patches(gen_ftrs, patch_sz)\n",
    "    n_gen_patches = gen_patches.size()[0]\n",
    "\n",
    "    # size: (n_patches, 1)\n",
    "    gen_patches_norm = gen_patches.view(n_gen_patches, -1).norm(dim=1, keepdim=True)\n",
    "    style_patches_norm = style_patches.view(n_style_patches, -1).norm(dim=1, keepdim=True)\n",
    "\n",
    "    # size: (n_gen_patches, n_style_patches)\n",
    "    # row `i` contains a measure of the similarity between patch `i` of\n",
    "    # ftrs(generated image) and every patch of ftrs(style image[s])\n",
    "    # (conv out size is (n_style_imgs, n_gen_patches, sqrt(n_style_patches_per_img), sqrt(n_style_patches_per_img)),\n",
    "    # so we need to swap the first two dimensions and resize)\n",
    "    patches_correlations = (F.conv2d(style_ftrs, weight=gen_patches).permute(1, 0, 2, 3).reshape(n_gen_patches, n_style_patches) /\n",
    "                           (gen_patches_norm @ style_patches_norm.t()))\n",
    "\n",
    "    idx_best_matches = patches_correlations.argmax(dim=-1)\n",
    "    # referred to as NN(i) in the paper, size (n_patches)\n",
    "    # row `i` contains the best match found, between the patches extracted\n",
    "    # from ftrs(style image), for the patch `i` extracted from ftrs(gen image)\n",
    "    best_matches = style_patches[idx_best_matches]\n",
    "\n",
    "    # it's doing sqrt before my sqr undoes it, maybe it's better not to use norm()\n",
    "    # and do it manually\n",
    "    return (gen_patches - best_matches).norm()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WP0lMymJenHD"
   },
   "outputs": [],
   "source": [
    "class FeaturesCalculator:\n",
    "    def __init__(self, vgg_style_layers_idx:List[int], vgg_content_layers_idx:List[int],\n",
    "                 vgg:nn.Module=None, normalize_inputs=False):\n",
    "        self.vgg = vgg19(pretrained=True) if vgg is None else vgg\n",
    "        self.vgg.eval()\n",
    "        modules_to_hook = [self.vgg.features[idx] for idx in (*vgg_style_layers_idx, *vgg_content_layers_idx)]\n",
    "        self.hooks = hook_outputs(modules_to_hook, detach=False)\n",
    "        self.style_ftrs_hooks = self.hooks[:len(vgg_style_layers_idx)]\n",
    "        self.content_ftrs_hooks = self.hooks[len(vgg_style_layers_idx):]\n",
    "        self.normalize_inputs = normalize_inputs\n",
    "        # TODO: when to remove hooks??? no destructor in Python right?\n",
    "        #  `clean` method????\n",
    "    \n",
    "    def _get_hooks_out(self, hooks):\n",
    "        return [h.stored for h in hooks]\n",
    "    \n",
    "    def _forward(img_t:torch.Tensor):\n",
    "        if self.normalize_inputs: \n",
    "            mean, std = fastai.vision.imagenet_stats\n",
    "            img_t = fastai.vision.normalize(img_t, torch.tensor(mean), torch.tensor(std))\n",
    "        self.vgg(img_t)\n",
    "    \n",
    "    def calc_style(self, img_t:torch.Tensor) -> List[torch.Tensor]:\n",
    "        self.vgg(img_t)\n",
    "        return self._get_hooks_out(self.style_ftrs_hooks)\n",
    "    \n",
    "    def calc_content(self, img_t:torch.Tensor) -> List[torch.Tensor]:\n",
    "        self.vgg(img_t)\n",
    "        return self._get_hooks_out(self.content_ftrs_hooks)\n",
    "    \n",
    "    def calc_style_and_content(self, img_t:torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        self.vgg(img_t)\n",
    "        style_ftrs = self._get_hooks_out(self.style_ftrs_hooks)\n",
    "        content_ftrs = self._get_hooks_out(self.content_ftrs_hooks)\n",
    "        return style_ftrs, content_ftrs\n",
    "\n",
    "\n",
    "def calc_loss(gen_img_t:torch.Tensor, gen_style_ftrs:torch.Tensor, gen_content_ftrs:torch.Tensor, \n",
    "              target_style_ftrs: torch.Tensor, target_content_ftrs:torch.Tensor, \n",
    "              style_patches:torch.Tensor, loss_weights:LossWeights=None) -> torch.Tensor:\n",
    "    if loss_weights is None: loss_weights = LossWeights()\n",
    "    \n",
    "    # Iterate over feature maps produced by different cnn layers\n",
    "    s_loss = torch.tensor(0.)\n",
    "    if loss_weights.style > 0.:\n",
    "        for i, gen_style_ftr_map in enumerate(gen_style_ftrs):\n",
    "            s_loss += style_loss(gen_style_ftr_map, target_style_ftrs[i], style_patches[i])\n",
    "        assert s_loss.requires_grad\n",
    "\n",
    "    c_loss = torch.tensor(0.)\n",
    "    if loss_weights.content > 0.:\n",
    "        for i, gen_content_ftr_map in enumerate(gen_content_ftrs):\n",
    "            c_loss += content_loss(gen_content_ftr_map, target_content_ftrs[i])\n",
    "        assert c_loss.requires_grad\n",
    "\n",
    "    reg = smoothness_reg(gen_img_t) if loss_weights.reg > 0 else torch.tensor(0.)\n",
    "\n",
    "    loss = loss_weights.style * s_loss + loss_weights.content * c_loss + loss_weights.reg * reg\n",
    "    assert loss.requires_grad\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRRVKhLkenHG"
   },
   "source": [
    "# INPUTS MANAGEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iarCyDlnenHH"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/mf1024/ImageNet-Datasets-Downloader.git \"C:/Users/blabla/ImageNetDownloader\"\n",
    "def download_imagenet_subset():\n",
    "    !python C:/Users/blabla/ImageNetDownloader/downloader.py \\\n",
    "        -data_root C:/Users/blabla/imagenet \\\n",
    "        -number_of_classes 5 \\\n",
    "        -images_per_class 10\n",
    "    \n",
    "def get_img_from_url(url) -> PIL.Image.Image:\n",
    "    import requests\n",
    "    return PIL.Image.open(requests.get(url, stream=True).raw)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXC70x2us1ZL"
   },
   "outputs": [],
   "source": [
    "def get_transformed_style_imgs(img:PIL.Image):\n",
    "    fast_img = fastTF.Image(TF.to_tensor(img))\n",
    "    imgs = []\n",
    "    for scale in (0.85, 0.9, 0.95, 1, 1.05, 1.1, 1.15):\n",
    "        for rotation in (-15, -7.5, 0, 7.5, 15):\n",
    "            new_img = fast_img.apply_tfms([fastTF.rotate(degrees=rotation), fastTF.zoom(scale=scale)], \n",
    "                                          size=img_sz, \n",
    "                                          resize_method=fastai.vision.ResizeMethod.PAD, \n",
    "                                          padding_mode='zeros')\n",
    "            imgs.append(fast_img_to_tensor(new_img))\n",
    "    return torch.cat(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XijATYk_enHJ"
   },
   "outputs": [],
   "source": [
    "def img_t_from_url(url, target_sz:int=img_sz) -> torch.Tensor:\n",
    "    img = get_img_from_url(url)\n",
    "    return img_to_tensor(img, target_sz)\n",
    "    \n",
    "def img_t_from_path(path) -> torch.Tensor:\n",
    "    img = PIL.Image.open(path)\n",
    "    return img_to_tensor(img)\n",
    "\n",
    "def img_to_tensor(img:PIL.Image.Image, target_sz:int=img_sz) -> torch.Tensor:\n",
    "    target_sz_2d = (target_sz, target_sz)\n",
    "    if img.width > img.height:\n",
    "        img = TF.pad(img, padding=(0, (img.width - img.height)//2))\n",
    "    elif img.height > img.width:\n",
    "        img = TF.pad(img, padding=((img.height - img.width)//2, 0))\n",
    "    if img.size != target_sz_2d: img = TF.resize(img, target_sz_2d)\n",
    "    x = TF.to_tensor(img)\n",
    "    x.unsqueeze_(0)\n",
    "    return x\n",
    "\n",
    "def fast_img_to_tensor(img:fastai.vision.Image) -> torch.Tensor:\n",
    "    return img.px.unsqueeze(0)\n",
    "\n",
    "def check_norm_is_needed_vgg(x):\n",
    "    stats = (torch.Tensor([0.485, 0.456, 0.406]), \n",
    "             torch.Tensor([0.229, 0.224, 0.225]))\n",
    "    out = vgg(fastai.vision.normalize(x, *stats))\n",
    "    out2 = vgg(x)\n",
    "    return out.max(), out.argmax(), out2.max(), out2.argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpDtMkMFenHN"
   },
   "outputs": [],
   "source": [
    "def normalize(img_t:torch.Tensor):\n",
    "    mean, std = fastai.vision.imagenet_stats\n",
    "    return fastai.vision.normalize(img_t, torch.tensor(mean), torch.tensor(std))\n",
    "    \n",
    "def denormalize(img_t:torch.Tensor):\n",
    "    mean, std = fastai.vision.imagenet_stats\n",
    "    return fastai.vision.denormalize(img_t, torch.tensor(mean), torch.tensor(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageURLs:\n",
    "    PAINTING = 'https://www.moma.org/media/W1siZiIsIjQ2NzUxNyJdLFsicCIsImNvbnZlcnQiLCItcXVhbGl0eSA5MCAtcmVzaXplIDIwMDB4MjAwMFx1MDAzZSJdXQ.jpg?sha=314ebf8cc678676f'\n",
    "    BASKET_BALL = 'https://miro.medium.com/proxy/1*BDE-SkJBCG_7P4chK4vKnw.jpeg'\n",
    "    FERRARI_F1 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Ferrari_F1_2006_EMS.jpg/1024px-Ferrari_F1_2006_EMS.jpg'\n",
    "    RENAULT_F1 = 'https://upload.wikimedia.org/wikipedia/commons/3/31/Renault_F1_front_IAA_2005.jpg'\n",
    "    ELON_MUSK = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Elon_Musk_2015.jpg/408px-Elon_Musk_2015.jpg'\n",
    "    MARK_ZUCK = 'https://live.staticflickr.com/6156/6198197101_9d7a685618_b.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_img_t = normalize(img_t_from_url(ImageURLs.PAINTING))\n",
    "basket_ball_img_t = normalize(img_t_from_url(ImageURLs.BASKET_BALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai.vision.show_image(denormalize(basket_ball_img_t[0]))\n",
    "fastai.vision.show_image(denormalize(painting_img_t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_imgs_t = normalize(get_transformed_style_imgs(get_img_from_url(ImageURLs.PAINTING)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcXkSPQ1enHP"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    lr:float=1e-4\n",
    "    wd:float=0.\n",
    "    adam_betas:Tuple[float, float]=(0.9, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(style_img_t:torch.Tensor, content_img_t:torch.Tensor, init_gen_img_t:torch.Tensor=None,\n",
    "          n_iters=100, hyperparams:HyperParams=None, loss_weights:LossWeights=None, \n",
    "          progress_tracker:ProgressTracker=None, callbacks:List[Callable]=None) -> torch.Tensor:\n",
    "    gen_img_t = (init_gen_img_t if init_gen_img_t is not None\n",
    "                 else normalize(torch.rand(content_img_t.size())))\n",
    "    gen_img_t.requires_grad_(True)\n",
    "    if hyperparams is None: hyperparams = HyperParams()\n",
    "    opt = torch.optim.Adam([gen_img_t], lr=hyperparams.lr, betas=hyperparams.adam_betas, \n",
    "                           weight_decay=hyperparams.wd)\n",
    "    ftrs_calc = FeaturesCalculator(vgg_style_layers_idx, \n",
    "                                   vgg_content_layers_idx)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        target_style_ftrs = ftrs_calc.calc_style(style_img_t)\n",
    "        target_content_ftrs = ftrs_calc.calc_content(content_img_t)\n",
    "    style_patches = [split_in_patches(ftr_map, patch_sz) for ftr_map in target_style_ftrs]\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        gen_style_ftrs, gen_content_ftrs = ftrs_calc.calc_style_and_content(gen_img_t)\n",
    "\n",
    "        loss = calc_loss(gen_img_t, gen_style_ftrs, gen_content_ftrs, target_style_ftrs, \n",
    "                         target_content_ftrs, style_patches, loss_weights)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if callbacks is not None: \n",
    "            for c in callbacks: c(i, gen_img_t, loss)\n",
    "        if progress_tracker is not None: progress_tracker.notify(f'Completed iteration {i}')\n",
    "        \n",
    "    return gen_img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_progressive_growing(style_img:PIL.Image.Image, content_img:PIL.Image.Image, target_sz:int,\n",
    "                              init_sz:int=16, upsample_mode='bilinear', transform_style_img=True,\n",
    "                              n_iters_by_sz:int=200, **train_kwargs) -> torch.Tensor:\n",
    "    assert init_sz <= target_sz\n",
    "    cur_sz = init_sz\n",
    "    gen_img_t = None\n",
    "    while cur_sz <= target_sz:\n",
    "        cur_sz = min(cur_sz, target_sz)\n",
    "        if cur_sz != init_sz: \n",
    "            gen_img_t = F.interpolate(gen_img_t.detach(), cur_sz, mode=upsample_mode, align_corners=False)\n",
    "        style_img_t = (get_transformed_style_imgs(style_img, cur_sz) if transform_style_img\n",
    "                       else normalize(img_to_tensor(style_img, cur_sz)))\n",
    "        content_img_t = normalize(img_to_tensor(content_img, cur_sz))\n",
    "        gen_img_t = train(style_img_t, content_img_t, gen_img_t, n_iters=n_iters_by_sz,\n",
    "                          **train_kwargs)\n",
    "        cur_sz *= 2\n",
    "\n",
    "    return gen_img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZdXvCGpX6eH"
   },
   "outputs": [],
   "source": [
    "def get_print_results_callback(n_iters_between:int, n_total_iters:int, show_sz=False):\n",
    "    n_imgs = n_total_iters // n_iters_between\n",
    "    n_cols = 3\n",
    "    n_rows = n_imgs//n_cols + min(1, n_imgs % n_cols)\n",
    "    imgs, losses = [], []\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 16 * n_rows/n_cols))\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs.flatten(): ax.axis('off')\n",
    "    display_id = None\n",
    "\n",
    "    def _print_result(i, gen_img_t, loss):\n",
    "        if (i+1) % n_iters_between == 0:\n",
    "            gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "            gen_img = fastai.vision.Image(gen_img_t)          \n",
    "            imgs.append(gen_img)\n",
    "            losses.append(loss.detach().cpu())\n",
    "            for j, img in enumerate(imgs):\n",
    "                # If using progressive growing, it's only right if the number of iterations \n",
    "                # by size is a multiple of `n_iters_between`\n",
    "                iter_idx = (j+1) * n_iters_between\n",
    "                title = f'Iteration {iter_idx}, loss = {losses[j]}'\n",
    "                if show_sz: title += f', size = {img.size}'\n",
    "                img.show(ax=axs[j], title=title)\n",
    "            #plt.close()\n",
    "            nonlocal display_id\n",
    "            if display_id is None: display_id = display.display(fig, display_id=True)\n",
    "            display_id.update(fig)\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "    return _print_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "P3HaOyfrhgRS",
    "outputId": "b0a83833-25b6-4ecf-da49-4ffc96a1402f"
   },
   "outputs": [],
   "source": [
    "n_iters=200\n",
    "gen_img_t = train(painting_img_t, \n",
    "                  basket_ball_img_t,\n",
    "                  hyperparams=HyperParams(lr=0.1), \n",
    "                  n_iters=n_iters, \n",
    "                  loss_weights=LossWeights(style=0.1, content=1., reg=0.),\n",
    "                  callbacks=[get_print_results_callback(20, n_iters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "kvjBe3G7enHR",
    "outputId": "d53ea96a-c8ed-450b-e87e-c5e9d7de2360"
   },
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpoRRk_k244B"
   },
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "D0RwdA4r2mc2",
    "outputId": "7c9ea08b-be35-4f2f-de6f-b28741109151"
   },
   "outputs": [],
   "source": [
    "gen_img_t.min(), gen_img_t.max(), gen_img_t.size(), gen_img_t.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wqejd6ogB1hK",
    "outputId": "eb38fd3f-75be-413b-d849-1886c0175723"
   },
   "outputs": [],
   "source": [
    "denorm_gen_img_t = denormalize(gen_img_t.detach())\n",
    "pix_dist = {0: 0}\n",
    "for px in denorm_gen_img_t.flatten():\n",
    "    if 0. <= px <= 1.: \n",
    "        pix_dist[0] += 1\n",
    "    else:\n",
    "        key = int(px.item()//1)\n",
    "        if key not in pix_dist: pix_dist[key] = 0\n",
    "        pix_dist[key] += 1\n",
    "\n",
    "pix_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some pixels out of range. Maybe they would be black/white anyway and thinking about a solution (like penalizing it with loss func) is not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with progressive growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = get_img_from_url(ImageURLs.PAINTING)\n",
    "content_img = get_img_from_url(ImageURLs.BASKET_BALL)\n",
    "n_iters_by_sz = 200\n",
    "init_sz, target_sz = 64, 256\n",
    "n_total_iters = n_iters_by_sz * int(1 + math.log2(target_sz//init_sz))\n",
    "gen_img_t = train_progressive_growing(style_img, \n",
    "                                      content_img, \n",
    "                                      target_sz,\n",
    "                                      init_sz=init_sz,\n",
    "                                      transform_style_img=False,\n",
    "                                      n_iters_by_sz=n_iters_by_sz, \n",
    "                                      hyperparams=HyperParams(lr=0.1),\n",
    "                                      loss_weights=LossWeights(style=0.1, content=1., reg=1e-3),\n",
    "                                      callbacks=[get_print_results_callback(40, n_total_iters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_clamped_gen_img_t = denormalize(gen_img_t.detach().cpu()).squeeze(dim=0).clamp(0, 1)\n",
    "gen_img = fastai.vision.Image(denorm_clamped_gen_img_t)\n",
    "gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img.save('mrfcnn_tr1_200it_pg64-256.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHNcWMMaenHT"
   },
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbhhFA0DenHT",
    "outputId": "81b6163b-a64f-402f-f9f1-ee3f7ac4d2a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_content_loss():\n",
    "    x = torch.Tensor([[[0]*4]*4]*3)\n",
    "    y = torch.Tensor([[[1]*4]*4]*3)\n",
    "    loss1 = content_loss(x, y)\n",
    "    x += 0.5\n",
    "    loss2 = content_loss(x, y)\n",
    "    y -= 0.25\n",
    "    loss3 = content_loss(x, y)\n",
    "    assert loss1 == 4*4*3\n",
    "    assert loss2 == 4*3\n",
    "    assert loss3 == 3\n",
    "\n",
    "def test_smoothness_reg():\n",
    "    uniform_img = torch.Tensor([[[1]*4]*4]*3)\n",
    "    print(uniform_img.size())\n",
    "    diffy_x_img = torch.Tensor([[[1]*4, [0]*4]*2]*3)\n",
    "    diffy_y_img = torch.Tensor([[[1, 0]*2]*4]*3)\n",
    "    diffy_x_y_img = torch.Tensor([[[1, 0]*2, [0, 1]*2]*2]*3)\n",
    "    \n",
    "    print(diffy_x_y_img)\n",
    "    assert smoothness_reg(uniform_img) == 0\n",
    "    assert smoothness_reg(diffy_x_img) == 3*3*4\n",
    "    assert smoothness_reg(diffy_y_img) == 3*4*3\n",
    "    assert smoothness_reg(diffy_x_y_img) == 2*3*4*3\n",
    "\n",
    "def test_split_in_patches():\n",
    "    img = torch.Tensor([i for i in range(3*4*4)]).view(3, 4, 4)\n",
    "    # tensor([[[ 0.,  1.,  2.,  3.],\n",
    "    #          [ 4.,  5.,  6.,  7.],\n",
    "    #          [ 8.,  9., 10., 11.],\n",
    "    #          [12., 13., 14., 15.]],\n",
    "\n",
    "    #         [[16., 17., 18., 19.],\n",
    "    #          [20., 21., 22., 23.],\n",
    "    #          [24., 25., 26., 27.],\n",
    "    #          [28., 29., 30., 31.]],\n",
    "\n",
    "    #         [[32., 33., 34., 35.],\n",
    "    #          [36., 37., 38., 39.],\n",
    "    #          [40., 41., 42., 43.],\n",
    "    #          [44., 45., 46., 47.]]])\n",
    "    actual_2x2 = split_in_patches(img, patch_sz=2)\n",
    "    actual_3x3 = split_in_patches(img, patch_sz=3)\n",
    "    expected_2x2 = torch.Tensor(\n",
    "       [[[[ 0.,  1.],\n",
    "          [ 4.,  5.]],\n",
    "\n",
    "         [[16., 17.],\n",
    "          [20., 21.]],\n",
    "\n",
    "         [[32., 33.],\n",
    "          [36., 37.]]],\n",
    "\n",
    "\n",
    "        [[[ 1.,  2.],\n",
    "          [ 5.,  6.]],\n",
    "\n",
    "         [[17., 18.],\n",
    "          [21., 22.]],\n",
    "\n",
    "         [[33., 34.],\n",
    "          [37., 38.]]],\n",
    "\n",
    "\n",
    "        [[[ 2.,  3.],\n",
    "          [ 6.,  7.]],\n",
    "\n",
    "         [[18., 19.],\n",
    "          [22., 23.]],\n",
    "\n",
    "         [[34., 35.],\n",
    "          [38., 39.]]],\n",
    "\n",
    "\n",
    "        [[[ 4.,  5.],\n",
    "          [ 8.,  9.]],\n",
    "\n",
    "         [[20., 21.],\n",
    "          [24., 25.]],\n",
    "\n",
    "         [[36., 37.],\n",
    "          [40., 41.]]],\n",
    "\n",
    "\n",
    "        [[[ 5.,  6.],\n",
    "          [ 9., 10.]],\n",
    "\n",
    "         [[21., 22.],\n",
    "          [25., 26.]],\n",
    "\n",
    "         [[37., 38.],\n",
    "          [41., 42.]]],\n",
    "\n",
    "\n",
    "        [[[ 6.,  7.],\n",
    "          [10., 11.]],\n",
    "\n",
    "         [[22., 23.],\n",
    "          [26., 27.]],\n",
    "\n",
    "         [[38., 39.],\n",
    "          [42., 43.]]],\n",
    "\n",
    "\n",
    "        [[[ 8.,  9.],\n",
    "          [12., 13.]],\n",
    "\n",
    "         [[24., 25.],\n",
    "          [28., 29.]],\n",
    "\n",
    "         [[40., 41.],\n",
    "          [44., 45.]]],\n",
    "\n",
    "\n",
    "        [[[ 9., 10.],\n",
    "          [13., 14.]],\n",
    "\n",
    "         [[25., 26.],\n",
    "          [29., 30.]],\n",
    "\n",
    "         [[41., 42.],\n",
    "          [45., 46.]]],\n",
    "\n",
    "\n",
    "        [[[10., 11.],\n",
    "          [14., 15.]],\n",
    "\n",
    "         [[26., 27.],\n",
    "          [30., 31.]],\n",
    "\n",
    "         [[42., 43.],\n",
    "          [46., 47.]]]])\n",
    "    expected_3x3=torch.Tensor(\n",
    "       [[[[ 0.,  1.,  2.],\n",
    "          [ 4.,  5.,  6.],\n",
    "          [ 8.,  9., 10.]],\n",
    "\n",
    "         [[16., 17., 18.],\n",
    "          [20., 21., 22.],\n",
    "          [24., 25., 26.]],\n",
    "\n",
    "         [[32., 33., 34.],\n",
    "          [36., 37., 38.],\n",
    "          [40., 41., 42.]]],\n",
    "\n",
    "\n",
    "        [[[ 1.,  2.,  3.],\n",
    "          [ 5.,  6.,  7.],\n",
    "          [ 9., 10., 11.]],\n",
    "\n",
    "         [[17., 18., 19.],\n",
    "          [21., 22., 23.],\n",
    "          [25., 26., 27.]],\n",
    "\n",
    "         [[33., 34., 35.],\n",
    "          [37., 38., 39.],\n",
    "          [41., 42., 43.]]],\n",
    "\n",
    "\n",
    "        [[[ 4.,  5.,  6.],\n",
    "          [ 8.,  9., 10.],\n",
    "          [12., 13., 14.]],\n",
    "\n",
    "         [[20., 21., 22.],\n",
    "          [24., 25., 26.],\n",
    "          [28., 29., 30.]],\n",
    "\n",
    "         [[36., 37., 38.],\n",
    "          [40., 41., 42.],\n",
    "          [44., 45., 46.]]],\n",
    "\n",
    "\n",
    "        [[[ 5.,  6.,  7.],\n",
    "          [ 9., 10., 11.],\n",
    "          [13., 14., 15.]],\n",
    "\n",
    "         [[21., 22., 23.],\n",
    "          [25., 26., 27.],\n",
    "          [29., 30., 31.]],\n",
    "\n",
    "         [[37., 38., 39.],\n",
    "          [41., 42., 43.],\n",
    "          [45., 46., 47.]]]])\n",
    "    assert(torch.equal(actual_2x2, expected_2x2))\n",
    "    assert(torch.equal(actual_3x3, expected_3x3))\n",
    "\n",
    "def test_style_loss(vgg19):\n",
    "    elon_photo_url = 'https://wonderfulengineering.com/wp-content/uploads/2018/09/musk5.jpg'\n",
    "    elon_drawing_url = 'https://i.redd.it/ofljkrzi82r21.jpg'\n",
    "    rectangular_spain_flag_url = 'https://upload.wikimedia.org/wikipedia/commons/d/d5/Flag_of_Spain_%28WFB_2000%29.jpg'\n",
    "    #'http://icons.iconarchive.com/icons/wikipedia/flags/256/ES-Spain-Flag-icon.png'\n",
    "    #hand_spain_flag_url = 'https://cdn.pixabay.com/photo/2015/02/16/00/20/spain-637843_960_720.jpg'\n",
    "    circular_spain_flag_url = 'https://cdn.pixabay.com/photo/2017/10/04/10/44/spain-2815785_960_720.jpg'\n",
    "    #'http://files.softicons.com/download/web-icons/world-cup-flags-icons-by-custom-icon-design/png/64x64/Spain.png'\n",
    "    austria_flag_url = 'https://pixnio.com/free-images/flags-of-the-world/flag-of-austria-725x483.jpg'\n",
    "    #'https://www.publicdomainpictures.net/pictures/120000/velka/austria-flag.jpg'\n",
    "    ftrs_calc = FeaturesCalculator(vgg_style_layers_idx, vgg_content_layers_idx, vgg19)\n",
    "    url_to_ftrs = lambda url: ftrs_calc.calc_style(img_t_from_url(url))\n",
    "\n",
    "    ftr_map_1 = torch.rand(1, 16, 8, 8)\n",
    "    ftr_map_2 = torch.rand(1, 16, 8, 8)    \n",
    "    ftr_map_sp_rect = url_to_ftrs(rectangular_spain_flag_url)[0]\n",
    "    ftr_map_sp_circ = url_to_ftrs(circular_spain_flag_url)[0]\n",
    "    ftr_map_austria = url_to_ftrs(austria_flag_url)[0]\n",
    "    ftr_map_sp_rect_patches = split_in_patches(ftr_map_sp_rect)    \n",
    "    \n",
    "    loss_rect_circ = style_loss(ftr_map_sp_circ, ftr_map_sp_rect, ftr_map_sp_rect_patches)\n",
    "    loss_rect_sp_aust = style_loss(ftr_map_austria, ftr_map_sp_rect, ftr_map_sp_rect_patches)\n",
    "    loss_equal = style_loss(ftr_map_1, ftr_map_1)\n",
    "    loss_different = style_loss(ftr_map_1, ftr_map_2)\n",
    "    #assert loss_equal == 0, f'{loss_equal}'\n",
    "    #assert loss_different > 0, f'{loss_different}'\n",
    "    assert loss_rect_circ < loss_rect_sp_aust, f'{loss_rect_circ}, {loss_rect_sp_aust}'\n",
    "\n",
    "test_content_loss()\n",
    "test_smoothness_reg()\n",
    "test_split_in_patches()\n",
    "# This isn't so easy to test\n",
    "#test_style_loss(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhAYuNRTenHf"
   },
   "source": [
    "# PENDING\n",
    "\n",
    "* Prepare for GPU and test if it improves anything in terms of speed\n",
    "* Penalize gen_img being out of range (be aware range is different for each channel)\n",
    "* Check better if normalized cross-correlation is ok\n",
    "* Don't forget to release plots, be it plot.close() or whatever\n",
    "* How to deal with rectangular images? Right now, I'm resizing the larger dim to 224 and filling the rest with black padding. Other options are:\n",
    "  * Crop (even if random it shouldn't make much sense)\n",
    "  * Check/think if vgg19 is actually capable of dealing with input sizes other than (224, 224) while preserving evaluation quality\n",
    "* Fit one cycle may be used/adapted????\n",
    "* Add a reference to the paper https://arxiv.org/pdf/1601.04589.pdf and authors\n",
    "  \n",
    "# TRAINING TEST STEPS\n",
    "1. Use only content loss\n",
    "2. Check adding style loss improves the results\n",
    "    -Check requires_grad of tensors in the middle of the process\n",
    "3. Check adding regularizer smoothes the result\n",
    "\n",
    "# IMPROVEMENTS\n",
    "* Precalc as much stuff related to content and style image as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYo1iFmlenHf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gjw4-bKLUEAX",
    "BwbNdBVHNJff",
    "46I-sgt54f5A",
    "HGGwo7J29YWF",
    "Y6JeXXBLJlWh",
    "kHNcWMMaenHT",
    "Kmd4GyjpenHV"
   ],
   "name": "mrf_cnn_style_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "321a24087ca24c30a7a1fbb70522aa78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "63d193063c974ce38dd5658c1003f934": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7003d257b40d43468dedf56fbeac7d03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7263955811f047c0a17cb5cb242c79c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "804e589b8a5e4778a3c717017b0ab141": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f0ac62206cff454589d29df65f91f33f",
       "IPY_MODEL_8f512a2b726246b08d3bf4ab107c91da"
      ],
      "layout": "IPY_MODEL_b54f703adf1c48508f349610fa4eb11d"
     }
    },
    "8f512a2b726246b08d3bf4ab107c91da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7263955811f047c0a17cb5cb242c79c6",
      "placeholder": "​",
      "style": "IPY_MODEL_7003d257b40d43468dedf56fbeac7d03",
      "value": " 548M/548M [01:33&lt;00:00, 6.18MB/s]"
     }
    },
    "b54f703adf1c48508f349610fa4eb11d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0ac62206cff454589d29df65f91f33f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63d193063c974ce38dd5658c1003f934",
      "max": 574673361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_321a24087ca24c30a7a1fbb70522aa78",
      "value": 574673361
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
